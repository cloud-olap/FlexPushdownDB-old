{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "783a88ff-7c3d-42f7-b90b-51a00aeed538",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45f86996-4d4e-49e2-85a2-bf772cd97e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# queries = ['{}a'.format(i) for i in range(1, 34)]\n",
    "# queries = ['1a', '1b', '1c', '1d', '2a', '2b', '2c', '2d', '3a', '3b', '3c', '4a', '4b', '4c', '5a', '5b', '5c',\n",
    "#            '6a', '6b', '6c', '6d', '6e', '6f', '7a', '7b', '7c', '8a', '8b', '8c', '8d', '9a', '9b', '9c', '9d',\n",
    "#            '10a', '10b', '10c', '11a', '11b', '11c', '11d', '12a', '12b', '12c', '13a', '13b', '13c', '13d', '14a', '14b', '14c',\n",
    "#            '15a', '15b', '15c', '15d', '16a', '16b', '16c', '16d', '17a', '17b', '17c', '17d', '17e', '17f', '18a', '18b', '18c',\n",
    "#            '19a', '19b', '19c', '19d', '20a', '20b', '20c', '21a', '21b', '21c', '22a', '22b', '22c', '22d', '23a', '23b', '23c',\n",
    "#            '24a', '24b', '25a', '25b', '25c', '26a', '26b', '26c', '27a', '27b', '27c', '28a', '28b', '28c', '29a', '29b', '29c',\n",
    "#            '30a', '30b', '30c', '31a', '31b', '31c', '32a', '32b', '33a', '33b', '33c']\n",
    "queries = ['01', '02', '03', '04', '05-jo2', '06', '07', '08', '09', '10',\n",
    "           '11', '12', '13', '14', '15', '16', '17', '18', '19', '20',\n",
    "           '21', '22']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28024ced-d0af-44bb-a881-daece6d85463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_folder = 'job/original/'\n",
    "query_folder = 'tpch/original/'\n",
    "query_folder_tpch_05 = 'tpch/modified/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "278ac1e2-57c8-4925-8387-4be1ba54f875",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_prefix(lines, prefix):\n",
    "    for i in range(len(lines)):\n",
    "        if lines[i].startswith(prefix):\n",
    "            return i\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e67420b-bb51-465f-9b34-93fe161ba8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nth_occurrence(array, target, n):\n",
    "    return [i for i, ele in enumerate(array) if ele == target][n - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66c4e2b8-7d03-46bb-aecc-fc1cf6b2ca95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_log(fname):\n",
    "    # open file\n",
    "    with open(fname) as f:\n",
    "        lines = f.readlines()\n",
    "        \n",
    "    # metrics\n",
    "    metrics = {}\n",
    "    \n",
    "    # parse for each query\n",
    "    line_id = 0\n",
    "    for query in queries:\n",
    "        # point to the query\n",
    "        result_line_text = 'Result of \\'{}{}.sql\\' (id: 0)'.format(query_folder if query != '05-jo2' else query_folder_tpch_05, query)\n",
    "        line_id += find_prefix(lines[line_id:], result_line_text)\n",
    "        \n",
    "        # parse network\n",
    "        net_metrics_line_text = 'Bytes transferred totally '\n",
    "        line_id += find_prefix(lines[line_id:], net_metrics_line_text)\n",
    "        net_str = lines[line_id]\n",
    "        net = float(net_str[net_str.find('(')+1 : net_str.find('GB') - 1])\n",
    "        \n",
    "        # parse stage breakdown\n",
    "        pred_trans_metrics_line_text = 'Predicate Transfer Metrics |'\n",
    "        line_id += find_prefix(lines[line_id:], pred_trans_metrics_line_text)\n",
    "        table_start_line_id = line_id # start of detailed table metrics\n",
    "        pt_phase_str = lines[line_id+2]\n",
    "        join_phase_str = lines[line_id+3]\n",
    "        net_phase_str = lines[line_id+4]\n",
    "        other_phase_str = lines[line_id+5]\n",
    "        pt_perc = pt_phase_str[pt_phase_str.find('(')+1 : pt_phase_str.find(')')]\n",
    "        join_perc = join_phase_str[join_phase_str.find('(')+1 : join_phase_str.find(')')]\n",
    "        net_perc = net_phase_str[net_phase_str.find('(')+1 : net_phase_str.find(')')]\n",
    "        other_perc = other_phase_str[other_phase_str.find('(')+1 : other_phase_str.find(')')]\n",
    "        \n",
    "        # parse execution time\n",
    "        result_line_text = 'Result of \\'{}{}.sql\\' (id: 1)'.format(query_folder if query != '05-jo2' else query_folder_tpch_05, query)\n",
    "        time_line_text = 'Time (id: 1):'\n",
    "        line_id += find_prefix(lines[line_id:], result_line_text)\n",
    "        table_end_line_id = line_id # end of detailed table metrics\n",
    "        line_id += find_prefix(lines[line_id:], time_line_text)\n",
    "        time_str = lines[line_id][len(time_line_text): lines[line_id].find('secs')].strip()\n",
    "        time = float(time_str)\n",
    "        \n",
    "        # parse table size after pre-filtering\n",
    "        table_sizes = {}\n",
    "        id_to_type = {}\n",
    "        curr_line_id = table_start_line_id\n",
    "        while True:\n",
    "            # next table\n",
    "            next_line_gap = find_prefix(lines[curr_line_id:], 'Prephysical Op ID')\n",
    "            if next_line_gap == -1: break\n",
    "            curr_line_id += next_line_gap\n",
    "            if curr_line_id > table_end_line_id: break\n",
    "            \n",
    "            # parse\n",
    "            id_line = lines[curr_line_id][:-1]\n",
    "            table_line = lines[curr_line_id + 1][:-1]\n",
    "            type_line = lines[curr_line_id + 3][:-1]\n",
    "            num_rows_line = lines[curr_line_id + 4][:-1]\n",
    "            offset = id_line.find('[')\n",
    "            id = int(id_line[offset + 1 : -1])\n",
    "            table = table_line[offset:]\n",
    "            type = type_line[offset:]\n",
    "            num_rows = int(num_rows_line[offset:])\n",
    "            curr_line_id += 5\n",
    "            \n",
    "            # assemble metrics\n",
    "            if id in id_to_type and id_to_type[id] == 'Bloom Filter':\n",
    "                continue\n",
    "            table_sizes['{} [{}]'.format(table, id)] = {'type': type, \n",
    "                                                        'num_rows': num_rows}\n",
    "            id_to_type[id] = type\n",
    "        \n",
    "        # assemble metrics\n",
    "        one_metrics = {'time': time,\n",
    "                       'net': net,\n",
    "                       'pt_perc': pt_perc,\n",
    "                       'join_perc': join_perc,\n",
    "                       'network_perc': net_perc,\n",
    "                       'other_perc': other_perc,\n",
    "                       'table_sizes': table_sizes}\n",
    "        \n",
    "        metrics[query] = one_metrics\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "4c13bd3b-a9bd-403e-b15d-fb41f4f82c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write metrics into csv (bj, yk, pt of JOB)\n",
    "metrics_bj = parse_log('/Users/yyf/Desktop/temp/log-job-bj')\n",
    "metrics_yk = parse_log('/Users/yyf/Desktop/temp/log-job-yk')\n",
    "metrics_pt = parse_log('/Users/yyf/Desktop/temp/log-job-pt')\n",
    "\n",
    "ofile = '/Users/yyf/Desktop/temp/parse_out.csv'\n",
    "with open(ofile, 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile, delimiter=',', quoting=csv.QUOTE_MINIMAL)\n",
    "    \n",
    "    # execution time\n",
    "    writer.writerow(['Runtime (sec)'])\n",
    "    writer.writerow([])\n",
    "    writer.writerow(['job', 'bloom-join', 'yannakakis', 'pred-trans', 'bj / pt', 'yk / pt', '',\n",
    "                     'pt perc (pt)', 'join perc (pt)', 'other perc (pt)'])\n",
    "    for query in queries:\n",
    "        time_bj = metrics_bj[query]['time']\n",
    "        time_yk = metrics_yk[query]['time']\n",
    "        time_pt = metrics_pt[query]['time']\n",
    "        writer.writerow([query, time_bj, time_yk, time_pt, time_bj/time_pt, time_yk/time_pt, '',\n",
    "                        metrics_pt[query]['pt_perc'], metrics_pt[query]['join_perc'], metrics_pt[query]['other_perc']])\n",
    "    writer.writerow([])\n",
    "        \n",
    "    # table sizes\n",
    "    writer.writerow(['Joining table size'])\n",
    "    writer.writerow([])\n",
    "    for query in queries:\n",
    "        # init rows\n",
    "        title_row = [query, 'Table']\n",
    "        bj_row = ['# rows', 'bloom join']\n",
    "        yk_row = ['', 'yannakakis']\n",
    "        pt_row = ['', 'pred-trans']\n",
    "        \n",
    "        # get metrics and sorted keys\n",
    "        table_sizes_bj = metrics_bj[query]['table_sizes']\n",
    "        table_sizes_yk = metrics_yk[query]['table_sizes']\n",
    "        table_sizes_pt = metrics_pt[query]['table_sizes']\n",
    "        sorted_keys_bj = sorted(table_sizes_bj.keys())\n",
    "        sorted_keys_yk = sorted(table_sizes_yk.keys())\n",
    "        sorted_keys_pt = sorted(table_sizes_pt.keys())\n",
    "        title_row += sorted_keys_pt\n",
    "        bj_row += [table_sizes_bj[key]['num_rows'] for key in sorted_keys_bj]\n",
    "        yk_row += [table_sizes_yk[key]['num_rows'] for key in sorted_keys_yk]\n",
    "        pt_row += [table_sizes_pt[key]['num_rows'] for key in sorted_keys_pt]\n",
    "        \n",
    "        # write rows\n",
    "        writer.writerow(title_row)\n",
    "        writer.writerow(bj_row)\n",
    "        writer.writerow(yk_row)\n",
    "        writer.writerow(pt_row)\n",
    "        writer.writerow([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd957015-75e2-48db-a707-e00e9ef781fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write metrics into csv (bj, pt of TPC-H)\n",
    "sf = 100\n",
    "# queries = ['02', '03', '04', '07']\n",
    "metrics_bj = parse_log('/Users/yyf/Desktop/temp/log-tpch-bj-16bit')\n",
    "metrics_pt = parse_log('/Users/yyf/Desktop/temp/log-tpch-pt-16bit')\n",
    "\n",
    "ofile = '/Users/yyf/Desktop/temp/parse_out_16bit.csv'\n",
    "with open(ofile, 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile, delimiter=',', quoting=csv.QUOTE_MINIMAL)\n",
    "    \n",
    "    # execution time\n",
    "    writer.writerow(['Runtime (sec)'])\n",
    "    writer.writerow([])\n",
    "    writer.writerow(['tpch-sf{}'.format(sf), 'bloom-join', 'pred-trans', 'bj / pt', '', 'pt perc (pt)',\n",
    "                     'join perc (pt)', 'network perc (pt)', 'other perc (pt)'])\n",
    "    for query in queries:\n",
    "        if query == '01' or query == '06': continue\n",
    "        time_bj = metrics_bj[query]['time']\n",
    "        time_pt = metrics_pt[query]['time']\n",
    "        writer.writerow([query, round(time_bj,3), round(time_pt,3), round(time_bj/time_pt,3), '',\n",
    "                        metrics_pt[query]['pt_perc'], metrics_pt[query]['join_perc'],\n",
    "                        metrics_pt[query]['network_perc'], metrics_pt[query]['other_perc']])\n",
    "    writer.writerow([])\n",
    "        \n",
    "    # table sizes\n",
    "    writer.writerow(['Joining table size'])\n",
    "    writer.writerow([])\n",
    "    for query in queries:\n",
    "        if query == '01' or query == '06': continue\n",
    "        \n",
    "        # init rows\n",
    "        title_row = [query, 'Table']\n",
    "        bj_row = ['# rows', 'bloom join']\n",
    "        pt_row = ['', 'pred-trans']\n",
    "        \n",
    "        # get metrics and sorted keys\n",
    "        table_sizes_bj = metrics_bj[query]['table_sizes']\n",
    "        table_sizes_pt = metrics_pt[query]['table_sizes']\n",
    "        sorted_keys_bj = sorted(table_sizes_bj.keys())\n",
    "        sorted_keys_pt = sorted(table_sizes_pt.keys())\n",
    "        title_row += sorted_keys_pt\n",
    "        bj_row += [table_sizes_bj[key]['num_rows'] for key in sorted_keys_bj]\n",
    "        pt_row += [table_sizes_pt[key]['num_rows'] for key in sorted_keys_pt]\n",
    "        \n",
    "        # write rows\n",
    "        writer.writerow(title_row)\n",
    "        writer.writerow(bj_row)\n",
    "        writer.writerow(pt_row)\n",
    "        writer.writerow([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4ce93f37-386c-4e3f-bc4e-db7f995789bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write metrics into csv (dist-pt of TPC-H)\n",
    "sf = 100\n",
    "cluster = '4-r5.4-v2'\n",
    "dist_join = 'ption-dist-join'\n",
    "folder = '/Users/yyf/Desktop/temp/dist-PT/sf{}-{}/{}/'.format(sf, cluster, dist_join)\n",
    "metrics_no_pt = parse_log(folder + 'no-pt')\n",
    "metrics_bcast_val = parse_log(folder + 'bcast-val')\n",
    "metrics_ption_val = parse_log(folder + 'ption-val')\n",
    "metrics_ption_src_bf_dst_val = parse_log(folder + 'ption-src-bf-dst-val')\n",
    "metrics_ption_src_val_dst_bf = parse_log(folder + 'ption-src-val-dst-bf')\n",
    "# metrics_bcast_bf = parse_log(folder + 'bcast-bf')\n",
    "# metrics_adapt = parse_log(folder + 'adapt')\n",
    "# metrics_adapt_prune = parse_log(folder + 'adapt-prune')\n",
    "metrics_bcast_bf = parse_log(folder + 'parallel_dist_bf_merge/bcast-bf')\n",
    "metrics_adapt = parse_log(folder + 'parallel_dist_bf_merge/adapt')\n",
    "metrics_adapt_prune = parse_log(folder + 'parallel_dist_bf_merge/adapt-prune')\n",
    "\n",
    "ofile = folder + 'parse_out.csv'\n",
    "with open(ofile, 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile, delimiter=',', quoting=csv.QUOTE_MINIMAL)\n",
    "    \n",
    "    # execution time\n",
    "    writer.writerow(['Runtime (sec)'])\n",
    "    writer.writerow([])\n",
    "    writer.writerow(['tpch-sf{}'.format(sf), 'no-pt', 'bcast-val', 'bcast-bf', 'ption-val',\n",
    "                     'ption-src-bf-dst-val', 'ption-src-val-dst-bf', 'adapt', 'adapt-prune'])\n",
    "    for query in queries:\n",
    "        if query == '01' or query == '06': continue\n",
    "        time_no_pt = metrics_no_pt[query]['time']\n",
    "        time_bcast_val = metrics_bcast_val[query]['time']\n",
    "        time_bcast_bf = metrics_bcast_bf[query]['time']\n",
    "        time_ption_val = metrics_ption_val[query]['time']\n",
    "        time_ption_src_bf_dst_val = metrics_ption_src_bf_dst_val[query]['time']\n",
    "        time_ption_src_val_dst_bf = metrics_ption_src_val_dst_bf[query]['time']\n",
    "        time_adapt = metrics_adapt[query]['time']\n",
    "        time_adapt_prune = metrics_adapt_prune[query]['time']\n",
    "        if query == '05-jo2': query = '05'\n",
    "        writer.writerow([query, round(time_no_pt,3), round(time_bcast_val,3), round(time_bcast_bf,3),\n",
    "                        round(time_ption_val,3), round(time_ption_src_bf_dst_val,3), round(time_ption_src_val_dst_bf,3),\n",
    "                        round(time_adapt,3), round(time_adapt_prune,3)])\n",
    "    writer.writerow([])\n",
    "    \n",
    "    # network\n",
    "    writer.writerow(['Network (GB)'])\n",
    "    writer.writerow([])\n",
    "    writer.writerow(['tpch-sf{}'.format(sf), 'no-pt', 'bcast-val', 'bcast-bf', 'ption-val',\n",
    "                     'ption-src-bf-dst-val', 'ption-src-val-dst-bf', 'adapt', 'adapt-prune'])\n",
    "    for query in queries:\n",
    "        if query == '01' or query == '06': continue\n",
    "        net_no_pt = metrics_no_pt[query]['net']\n",
    "        net_bcast_val = metrics_bcast_val[query]['net']\n",
    "        net_bcast_bf = metrics_bcast_bf[query]['net']\n",
    "        net_ption_val = metrics_ption_val[query]['net']\n",
    "        net_ption_src_bf_dst_val = metrics_ption_src_bf_dst_val[query]['net']\n",
    "        net_ption_src_val_dst_bf = metrics_ption_src_val_dst_bf[query]['net']\n",
    "        net_adapt = metrics_adapt[query]['net']\n",
    "        net_adapt_prune = metrics_adapt_prune[query]['net']\n",
    "        if query == '05-jo2': query = '05'\n",
    "        writer.writerow([query, round(net_no_pt,3), round(net_bcast_val,3), round(net_bcast_bf,3),\n",
    "                        round(net_ption_val,3), round(net_ption_src_bf_dst_val,3), round(net_ption_src_val_dst_bf,3),\n",
    "                        round(net_adapt,3), round(net_adapt_prune,3)])\n",
    "    writer.writerow([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f750a0b5-b0db-4393-92d6-436be22443bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write metrics into csv (dist-pt of TPC-H)\n",
    "sf = 100\n",
    "cluster = '4-r5.4-v2'\n",
    "dist_join = 'adapt-dist-join'\n",
    "folder = '/Users/yyf/Desktop/temp/dist-PT/sf{}-{}/{}/'.format(sf, cluster, dist_join)\n",
    "metrics_no_pt = parse_log(folder + 'no-pt')\n",
    "metrics_adapt_prune0 = parse_log(folder + 'parallel_dist_bf_merge/adapt-prune')\n",
    "metrics_adapt_prune1 = parse_log(folder + 'parallel_dist_bf_merge/with_compute_cost_0.05/adapt-prune')\n",
    "metrics_adapt_prune2 = parse_log(folder + 'parallel_dist_bf_merge/with_compute_cost_0.1/adapt-prune')\n",
    "metrics_adapt_prune3 = parse_log(folder + 'parallel_dist_bf_merge/with_compute_cost_0.2/adapt-prune')\n",
    "metrics_adapt_prune4 = parse_log(folder + 'parallel_dist_bf_merge/with_compute_cost_0.5/adapt-prune')\n",
    "metrics_adapt_prune5 = parse_log(folder + 'parallel_dist_bf_merge/with_compute_cost_1/adapt-prune')\n",
    "\n",
    "ofile = folder + 'parse_out.csv'\n",
    "with open(ofile, 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile, delimiter=',', quoting=csv.QUOTE_MINIMAL)\n",
    "    \n",
    "    # execution time\n",
    "    writer.writerow(['Runtime (sec)'])\n",
    "    writer.writerow([])\n",
    "    writer.writerow(['tpch-sf{}'.format(sf), 'no-pt', 'adapt-prune', 'adapt-prune-c-0.05', 'adapt-prune-c-0.1', 'adapt-prune-c-0.2',\n",
    "                    'adapt-prune-c-0.5', 'adapt-prune-c-1'])\n",
    "    # writer.writerow(['tpch-sf{}'.format(sf), 'no-pt', 'adapt-prune'])\n",
    "    for query in queries:\n",
    "        if query == '01' or query == '06': continue\n",
    "        time_no_pt = metrics_no_pt[query]['time']\n",
    "        time_adapt_prune0 = metrics_adapt_prune0[query]['time']\n",
    "        time_adapt_prune1 = metrics_adapt_prune1[query]['time']\n",
    "        time_adapt_prune2 = metrics_adapt_prune2[query]['time']\n",
    "        time_adapt_prune3 = metrics_adapt_prune3[query]['time']\n",
    "        time_adapt_prune4 = metrics_adapt_prune4[query]['time']\n",
    "        time_adapt_prune5 = metrics_adapt_prune5[query]['time']\n",
    "        if query == '05-jo2': query = '05'\n",
    "        writer.writerow([query, round(time_no_pt,3), round(time_adapt_prune0,3), round(time_adapt_prune1,3), round(time_adapt_prune2,3),\n",
    "                         round(time_adapt_prune3,3), round(time_adapt_prune4,3), round(time_adapt_prune5,3)])\n",
    "        # writer.writerow([query, round(time_no_pt,3), round(time_adapt_prune0,3)])\n",
    "    writer.writerow([])\n",
    "    \n",
    "    # network\n",
    "    writer.writerow(['Network (GB)'])\n",
    "    writer.writerow([])\n",
    "    writer.writerow(['tpch-sf{}'.format(sf), 'no-pt', 'adapt-prune', 'adapt-prune-c-0.05', 'adapt-prune-c-0.1', 'adapt-prune-c-0.2',\n",
    "                    'adapt-prune-c-0.5', 'adapt-prune-c-1'])\n",
    "    # writer.writerow(['tpch-sf{}'.format(sf), 'no-pt', 'adapt-prune'])\n",
    "    for query in queries:\n",
    "        if query == '01' or query == '06': continue\n",
    "        net_no_pt = metrics_no_pt[query]['net']\n",
    "        net_adapt_prune0 = metrics_adapt_prune0[query]['net']\n",
    "        net_adapt_prune1 = metrics_adapt_prune1[query]['net']\n",
    "        net_adapt_prune2 = metrics_adapt_prune2[query]['net']\n",
    "        net_adapt_prune3 = metrics_adapt_prune3[query]['net']\n",
    "        net_adapt_prune4 = metrics_adapt_prune4[query]['net']\n",
    "        net_adapt_prune5 = metrics_adapt_prune5[query]['net']\n",
    "        if query == '05-jo2': query = '05'\n",
    "        writer.writerow([query, round(net_no_pt,3), round(net_adapt_prune0,3), round(net_adapt_prune1,3), round(net_adapt_prune2,3),\n",
    "                         round(net_adapt_prune3,3), round(net_adapt_prune4,3), round(net_adapt_prune5,3)])\n",
    "        # writer.writerow([query, round(net_no_pt,3), round(net_adapt_prune0,3)])\n",
    "    writer.writerow([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837a77b8-b9fd-4118-90d4-6d66aae9a3ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
